def generate_embedding(text: str) -> list:
    """Generate an embedding vector for the given text using OpenAI's API."""
    try:
        response = client.embeddings.create(
            model="text-embedding-ada-002",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        logger.error(f"Error generating embedding: {e}")
        raise

def generate_completion(query: str, context: str, system_prompt: str = None) -> str:
    """Generate a completion using OpenAI's chat API."""
    try:
        # Use default system prompt if none provided
        if not system_prompt:
            system_prompt = """VocÃª Ã© um assistente de IA que fornece resumos concisos baseados estritamente no contexto fornecido. 
            Abaixo estÃ¡ um termo-chave fornecido pelo usuÃ¡rio e um conjunto de artigos relacionados como contexto. 
            Sua tarefa Ã© gerar um resumo preciso e bem estruturado do termo-chave usando apenas as informaÃ§Ãµes presentes no contexto. 
            Se o contexto nÃ£o contiver informaÃ§Ãµes relevantes sobre o termo, informe que nenhuma informaÃ§Ã£o estÃ¡ disponÃ­vel.
            
            Formato de saÃ­da:
            T|Resumo do termo-chave caso o contexto contenha informaÃ§Ãµes sobre o termo-chave.
            F|Termo-chave nÃ£o encontrado no contexto.
            """

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Query: {query}\n\nContext: {context}"}
        ]

        response = client.chat.completions.create(
            model="gpt-4o",  # Using GPT-4 for better quality summaries
            messages=messages,
            temperature=0.2,
            # max_tokens=500
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error generating completion: {e}")
        raise

def generate_term_summary(title: str, content: str) -> str:
    """Generate a summary for an term using OpenAI's chat API."""
    try:
        messages = [
            {
                "role": "system",
                "content": "VocÃª Ã© um especialista em resumir artigos. Crie um resumo conciso do artigo, destacando os pontos-chave e detalhes importantes, com foco nas questÃµes ambientais e sociais da regiÃ£o amazÃ´nica, conforme abordadas pelo InfoAmazonia."
            },
            {
                "role": "user",
                "content": f"Title: {title}\n\nContent: {content}"
            }
        ]

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.5,
            # max_tokens=300
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error generating article summary: {e}")
        raise

def generate_article_summary(title: str, content: str, url: str) -> str:
    """Generate a summary for an article using OpenAI's chat API."""
    try:
        messages = [
            {
                "role": "system",
                "content": """VocÃª Ã© um especialista em resumos no estilo Smart Brevity. Crie um resumo conciso do artigo, formatado para o WhatsApp, utilizando Ã­cones para melhorar a leitura. Siga este formato:

ğŸ“ *Por que importa*: Explique em uma frase curta o impacto ambiental e social na regiÃ£o amazÃ´nica.

ğŸ”‘ *Pontos-chave*: Liste os principais fatos e dados do artigo do InfoAmazonia em atÃ© 3 bullets.

ğŸŒ *Contexto essencial*: ForneÃ§a informaÃ§Ãµes mÃ­nimas para entender o cenÃ¡rio sem sobrecarregar o leitor.

ğŸ”— Leia a reportagem completa em [infoamazonia.org]: Se houver, inclua um link ou sugestÃ£o para mais informaÃ§Ãµes. ex: http://aa109676-f2b5-40ce-9a8b-b7d95b3a219e-00-30gb0h9bugxba.spock.replit.dev/r/e6d6ba29, NÃƒO inclua a formataÃ§Ã£o markdown para link []()
"""
            },
            {
                "role": "user",
                "content": f"Title: {title}\n\nContent: {content}\n\nURL: {url}"
            }
        ]

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.5,
            # max_tokens=300
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error generating article summary: {e}")
        raise